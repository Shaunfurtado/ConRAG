import OpenAI from "openai";
import dotenv from "dotenv";
dotenv.config();

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export const analyzeEmailContext = async (emailContent: string) => {
  try {
    const stream = await client.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        {
          role: "system",
          content: "Analyze the email content and return only the classification category label: 'Interested', 'Not-Interested', or 'More-Information'.",
        },
        { role: "user", content: emailContent },
      ],
      max_tokens: 100,
      stream: true,
    });

    let result = "";
    for await (const chunk of stream) {
      result += chunk.choices[0]?.delta?.content || "";
    }
    console.log("Email classification result:", result.trim());
    return result.trim();
  } catch (error) {
    console.error("Error analyzing email context:", error);
  }
};

export const generateReply = async (emailContent: string, label: string) => {
  let context = "";

  switch (label) {
    case "Interested":
      context = "Write a reply to this email expressing interest in a demo call. Indicating that the user is interested in further information. Suggest a time slot for the call if necessary.";
      break;
    case "Not Interested":
      context = "Respond politely to this email, indicating that the user is not interested in further information.";
      break;
    case "More Information":
      context = "Provide additional information about the product or service in response to this email. Ask if the user has any other questions.";
      break;
    default:
      context = "Write a generic reply to this email, thanking the user for their interest.";
  }
  try {
    const stream = await client.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        { role: "system", content: context },
        { role: "user", content: emailContent },
      ],
      max_tokens: 100,
      stream: true,
    });

    let result = "";
    for await (const chunk of stream) {
      result += chunk.choices[0]?.delta?.content || "";
    }
    console.log("Generated reply:", result.trim());
    return result.trim();
  } catch (error) {
    console.error("Error generating reply:", error);
  }
};